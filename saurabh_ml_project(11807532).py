# -*- coding: utf-8 -*-
"""Saurabh_ML Project(11807532)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oq8ibS-X97rZIsBfK6mshimqbT25CULw
"""

import numpy as np # linear algebra
import matplotlib.pyplot  as plt # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


df = pd.read_csv("https://raw.githubusercontent.com/beingsaurabh/spam-email/master/emails.csv")
df.head()

df.isnull().sum()

df.describe()

df.corr()

X = df.iloc[:,1:3001]
X

Y = df.iloc[:,-1].values
Y

train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.25)

mnb = MultinomialNB(alpha=1.9)         # alpha by default is 1. alpha must always be > 0. 
# alpha is the '1' in the formula for Laplace Smoothing (P(words))
mnb.fit(train_x,train_y)
y_pred1 = mnb.predict(test_x)
print("Accuracy Score for Naive Bayes : ", accuracy_score(y_pred1,test_y))
print("Accuracy % for Naive Bayes     : ", 100*accuracy_score(y_pred1,test_y))

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

print("Confusion Matrix")
print(confusion_matrix(y_pred1,test_y))
print(classification_report(y_pred1,test_y))

svc = SVC(C=1.0,kernel='rbf',gamma='auto')         
# C here is the regularization parameter. Here, L2 penalty is used(default). It is the inverse of the strength of regularization.
# As C increases, model overfits.
# Kernel here is the radial basis function kernel.
# gamma (only used for rbf kernel) : As gamma increases, model overfits.
svc.fit(train_x,train_y)
y_pred2 = svc.predict(test_x)
print("Accuracy % for SVC : ", 100*accuracy_score(y_pred2,test_y))

print("Confusion Matrix")
print(confusion_matrix(y_pred2,test_y))
print(classification_report(y_pred2,test_y))

rfc = RandomForestClassifier(n_estimators=100,criterion='gini')
# n_estimators = No. of trees in the forest
# criterion = basis of making the decision tree split, either on gini impurity('gini'), or on infromation gain('entropy')
rfc.fit(train_x,train_y)
y_pred3 = rfc.predict(test_x)
print("Accuracy % of Random Forest Classifier : ", 100*accuracy_score(y_pred3,test_y))

print("Confusion Matrix")
print(confusion_matrix(y_pred3,test_y))
print(classification_report(y_pred3,test_y))

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn import model_selection
models = []
models.append(('NaiveBayes',MultinomialNB()))
models.append(('SupportVectorMachine',SVC()))
models.append((' RandomForestClassifier',RandomForestClassifier()))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=0)
	cv_results = model_selection.cross_val_score(model, train_x,train_y, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)
# boxplot algorithm comparison
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()